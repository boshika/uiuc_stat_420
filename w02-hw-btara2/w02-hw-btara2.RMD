---
title: "Week 2 - Homework"
author: "STAT 420, Summer 2021, Boshika Tara"
netid: "btara2"
date: '06-01-2021'
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---

## Exercise 1 (Using `lm`)

For this exercise we will use the `cats` dataset from the `MASS` package. You should use `?cats` to learn about the background of this dataset.

**(a)** Suppose we would like to understand the size of a cat's heart based on the body weight of a cat. Fit a simple linear model in `R` that accomplishes this task. Store the results in a variable called `cat_model`. Output the result of calling `summary()` on `cat_model`.
```{r}
cats <- MASS::cats
cats_model <- lm(Hwt ~ Bwt, data = cats)
summary(cats_model)
```
**(b)** Output only the estimated regression coefficients. Interpret $\hat{\beta_0}$ and $\beta_1$ in the *context of the problem*. Be aware that only one of those is an estimate.
```{r}
coef(cats_model)
```

**(c)** Use your model to predict the heart weight of a cat that weights **3.1** kg. Do you feel confident in this prediction? Briefly explain.
```{r}
predict(cats_model, data.frame(Bwt=3.1))
```
"Based on body weight of 3.1kg, the heart weight of a cat is 12.14893, I am pretty confident that this is an accurate prediction, as the value of 12.1 is an observed value in the dataset for the 3.1kg variable."

**(d)** Use your model to predict the heart weight of a cat that weights **1.5** kg. Do you feel confident in this prediction? Briefly explain.
```{r}
predict(cats_model, data.frame(Bwt=1.5))
```
"Based on body weight of 1.5kg, the heart weight of a cat is 5.694432, I am not confident that this is an accurate prediction, as this is an extrapolation"

**(e)** Create a scatterplot of the data and add the fitted regression line. Make sure your plot is well labeled and is somewhat visually appealing.
```{r}
library(ggplot2)
ggplot(cats) + 
  ylab("Cat's Height") +
  xlab("Cat's Weight(kgs)") +
  ggtitle("Cats Heights Correlation To Cats Body Weight") +
  geom_point(mapping = aes(x=Bwt, y=Hwt),
    shape=18, color="red") +
  geom_abline(slope = cats_model$coefficients[2],
              intercept = cats_model$coefficients[1])

```
**(f)** Report the value of $R^2$ for the model. Do so directly. Do not simply copy and paste the value from the full output in the console after running `summary()` in part **(a)**.
```{r}
summary(cats_model)$r.square
```

***

## Exercise 2 (Writing Functions)

This exercise is a continuation of Exercise 1.

**(a)** Write a function called `get_sd_est` that calculates an estimate of $\sigma$ in one of two ways depending on input to the function. The function should take three arguments as input:

- `fitted_vals` - A vector of fitted values from a model
- `actual_vals` - A vector of the true values of the response
- `mle` - A logical (`TRUE` / `FALSE`) variable which defaults to `FALSE`

The function should return a single value:

- $s_e$ if `mle` is set to `FALSE`.
- $\hat{\sigma}$ if `mle` is set to `TRUE`.
```{r}
get_sd_est <- function(fitted_vals, actual_vals, mle=FALSE) {
  model_residual <- actual_vals - fitted_vals
  length_s_e <- length(model_residual)-2
  length_sigma_hat <- length(model_residual)
  
  ifelse(mle == FALSE, sqrt(sum(model_residual^2)/ length_s_e), sqrt(sum(model_residual^2)/length_sigma_hat))
}
```
**(b)** Run the function `get_sd_est` on the residuals from the model in Exercise 1, with `mle` set to `FALSE`. Explain the resulting estimate in the context of the model.
```{r}
get_sd_est(cats_model$fitted.values, cats$Hwt)
```
**(c)** Run the function `get_sd_est` on the residuals from the model in Exercise 1, with `mle` set to `TRUE`. Explain the resulting estimate in the context of the model. Note that we are trying to estimate the same parameter as in part **(b)**.
```{r}
get_sd_est(cats_model$fitted.values, cats$Hwt, mle=TRUE)
```
**(d)** To check your work, output `summary(cat_model)$sigma`. It should match at least one of **(b)** or **(c)**.
```{r}
summary(cats_model)$sigma
```
***

## Exercise 3 (Simulating SLR)

Consider the model

\[
Y_i = 5 + -3 x_i + \epsilon_i
\]

with 

\[
\epsilon_i \sim N(\mu = 0, \sigma^2 = 10.24)
\]

where $\beta_0 = 5$ and $\beta_1 = -3$.

This exercise relies heavily on generating random observations. To make this reproducible we will set a seed for the randomization. Alter the following code to make `birthday` store your birthday in the format: `yyyymmdd`. For example, [William Gosset](https://en.wikipedia.org/wiki/William_Sealy_Gosset), better known as *Student*, was born on June 13, 1876, so he would use:

```{r}
birthday = 19990308
set.seed(birthday)
```
**(a)** Use `R` to simulate `n = 25` observations from the above model. For the remainder of this exercise, use the following "known" values of $x$.

```{r}
sim_slr = function(n, beta_0 = 10, beta_1 = -3, sigma = 1, xmin = 0, xmax = 10){
  x = runif(n, xmin, xmax)
  epsilon = rnorm(n, mean = 0, sd = sigma)
  y = beta_0 + beta_1 * x + epsilon
  data.frame(predictor = x, response = y)
}
rand_data <- sim_slr(n = 25, beta_0 = 5, beta_1 = -3, sigma = sqrt(10.24))
```

You may use [the `sim_slr ` function provided in the text](http://daviddalpiaz.github.io/appliedstats/simple-linear-regression.html#simulating-slr). Store the data frame this function returns in a variable of your choice. Note that this function calls $y$ `response` and $x$ `predictor`.
**(b)** Fit a model to your simulated data. Report the estimated coefficients. Are they close to what you would expect? Briefly explain.
```{r}
rand_model <- lm(response~predictor, data = rand_data)
coef(rand_model)
```
**Observation** 
The coefficient of β0 is 5.034745  which is very close to the true β0 value of 5.
The coefficient of $\hat{\beta_1}$ is -3.045775 which is close to true β1 value of -3 from the model.
This implies that the  fitted regression line will be very close to true line. We can run model multiple time and each time model will give a different line, which will will close but not exact.

**(c)** Plot the data you simulated in part **(a)**. Add the regression line from part **(b)** as well as the line for the true model. Hint: Keep all plotting commands in the same chunk.
```{r}
plot(response ~ predictor, data = rand_data,
     xlab = "Predictor Variable",
     ylab = "Response Variable",
     main = "Regression Data",
     pch  = 20,
     cex  = 1.5,
     col  = "darkorange")
abline(rand_model, lwd = 3, lty = 1, col = "darkred")
abline(5, -3, lwd = 3, lty = 2, col = "dodgerblue")
legend("topleft", c("Estimate", "Truth"), lty = c(1, 2), lwd = 2,
       col = c("darkred", "dodgerblue"))
```

**(d)** Use `R` to repeat the process of simulating `n = 25` observations from the above model $1500$ times. Each time fit a SLR model to the data and store the value of $\hat{\beta_1}$ in a variable called `beta_hat_1`. Some hints:
- Consider a `for` loop.
- Create `beta_hat_1` before writing the `for` loop. Make it a vector of length $1500$ where each element is `0`.
- Inside the body of the `for` loop, simulate new $y$ data each time. Use a variable to temporarily store this data together with the known $x$ data as a data frame.
- After simulating the data, use `lm()` to fit a regression. Use a variable to temporarily store this output.
- Use the `coef()` function and `[]` to extract the correct estimated coefficient.
- Use `beta_hat_1[i]` to store in elements of `beta_hat_1`.
- See the notes on [Distribution of a Sample Mean](http://daviddalpiaz.github.io/appliedstats/introduction-to-r.html#distribution-of-a-sample-mean) for some inspiration.

You can do this differently if you like. Use of these hints is not required.
```{r}
n <- 25
set.seed(19990308)
beta_hat_1 <- rep(0, 1500)
for(i in 1:1500){
  simulated_data <- sim_slr(n = 25, beta_0 = 5, beta_1 = -3, sigma = sqrt(10.24))
  simulated_model <- lm(response ~ predictor, data = simulated_data)
  beta_hat_1[i] <- coef(simulated_model)[2]
}
```
**(e)** Report the mean and standard deviation of `beta_hat_1`. Do either of these look familiar?
```{r}
mean(beta_hat_1)
sd(beta_hat_1)
```
**Observation** Mean of beta_hat_1 is  -2.99816 and standard deviation is 0.2362921. The mean of beta_hat_1 is very close to the actual beta_1 value.

**(f)** Plot a histogram of `beta_hat_1`. Comment on the shape of this histogram.
```{r}
hist(beta_hat_1, col = "darkred")
```

***

## Exercise 4 (Be a Skeptic)

Consider the model

\[
Y_i = 3 + 0 \cdot x_i + \epsilon_i
\]

with

\[
\epsilon_i \sim N(\mu = 0, \sigma^2 = 4)
\]

where $\beta_0 = 3$ and $\beta_1 = 0$.

Before answering the following parts, set a seed value equal to **your** birthday, as was done in the previous exercise.

```{r}
birthday = 19990308
set.seed(birthday)
```

**(a)** Use `R` to repeat the process of simulating `n = 75` observations from the above model $2500$ times. For the remainder of this exercise, use the following "known" values of $x$.

```{r}
n <- 75
x <- runif(n = 75, 0, 10)
beta_0 <- 3
beta_1 <- 0
sigma <- 2
beta_hat_1 <- rep(0,2500)
for(i in 1:2500) {
  eps <- rnorm(n,mean=0,sd=sigma)
  y <- beta_0 + beta_1 * x + eps
  simulated <- data.frame(predictor = x, response = y)
  model <- lm(y~x, data=simulated)
  beta_hat_1[i] <- coef(model)[2]
}
```

Each time fit a SLR model to the data and store the value of $\hat{\beta_1}$ in a variable called `beta_hat_1`. You may use [the `sim_slr ` function provided in the text](http://daviddalpiaz.github.io/appliedstats/simple-linear-regression.html#simulating-slr). Hint: Yes $\beta_1 = 0$.

**(b)** Plot a histogram of `beta_hat_1`. Comment on the shape of this histogram.
```{r}
hist(beta_hat_1, col="darkred")
```
**(c)** Import the data in [`skeptic.csv`](skeptic.csv) and fit a SLR model. The variable names in `skeptic.csv` follow the same convention as those returned by `sim_slr()`. Extract the fitted coefficient for $\beta_1$.
```{r}
skeptic <- read.csv("skeptic.csv")
skeptic_model <- lm(response~predictor, data=skeptic)
skeptic_beta_hat_1 <- coef(skeptic_model)[2]
skeptic_beta_hat_1
```
**(d)** Re-plot the histogram from **(b)**. Now add a vertical red line at the value of $\hat{\beta_1}$ in part **(c)**. To do so, you'll need to use `abline(v = c, col = "red")` where `c` is your value.
```{r}
hist(beta_hat_1, col="blue")
abline(v = skeptic_beta_hat_1, col="red")
```

**(e)** Your value of $\hat{\beta_1}$ in **(c)** should be negative. What proportion of the `beta_hat_1` values is smaller than your $\hat{\beta_1}$? Return this proportion, as well as this proportion multiplied by `2`.
```{r}
vals <- sum(beta_hat_1 < skeptic_beta_hat_1)
prop <- vals/length(beta_hat_1)
prop
```

**(f)** Based on your histogram and part **(e)**, do you think the [`skeptic.csv`](skeptic.csv) data could have been generated by the model given above? Briefly explain.

**Observation** Based on the histogram of beta_hat_1 and value of $\hat{\beta_1}$ from skeptic dataset, it looks like that skeptic data could have been generated by model but with low probability as the $\hat{\beta_1}$ for skeptic dataset is at a tail end. Also the proportion of beta_hat_1 values that are smaller than $\hat{\beta_1}$ are very small.

***

## Exercise 5 (Comparing Models)

For this exercise we will use the `Ozone` dataset from the `mlbench` package. You should use `?Ozone` to learn about the background of this dataset. You may need to install the `mlbench` package. If you do so, do not include code to install the package in your `R` Markdown document.

For simplicity, we will perform some data cleaning before proceeding.

```{r}
data(Ozone, package = "mlbench")
Ozone = Ozone[, c(4, 6, 7, 8)]
colnames(Ozone) = c("ozone", "wind", "humidity", "temp")
Ozone = Ozone[complete.cases(Ozone), ]
```

We have:

- Loaded the data from the package
- Subset the data to relevant variables
    - This is not really necessary (or perhaps a good idea) but it makes the next step easier
- Given variables useful names
- Removed any observation with missing values
    - This should be given much more thought in practice

For this exercise we will define the "Root Mean Square Error" of a model as

\[
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}.
\]

**(a)** Fit three SLR models, each with "ozone" as the response. For the predictor, use "wind speed," "humidity percentage," and "temperature" respectively. For each, calculate $\text{RMSE}$ and $R^2$. Arrange the results in a markdown table, with a row for each model. Suggestion: Create a data frame that stores the results, then investigate the `kable()` function from the `knitr` package.
```{r}
library(knitr)

model_wind <- lm(ozone~wind, data = Ozone)
model_humidity <- lm(ozone~humidity, data = Ozone)
model_temp <- lm(ozone~temp, data = Ozone) 

df <- data.frame( 
                  Model = c("Ozone_Wind","Ozone_Humidity","Ozone_Temp"),  
                  RMSE = c(
                            sqrt(sum(residuals(model_wind)^2)/length(residuals(model_wind))),
                            sqrt(sum(residuals(model_humidity)^2)/length(residuals(model_humidity))),
                            sqrt(sum(residuals(model_temp)^2)/length(residuals(model_temp)))
                          ),
             r_squared = c(
                            summary(model_wind)$r.squared,
                            summary(model_humidity)$r.squared,
                            summary(model_temp)$r.squared
                         ))

kable(df, caption = "Ozone Models Based on Wind, Humidity And Temperature")
```

**(b)** Based on the results, which of the three predictors used is most helpful for predicting ozone readings? Briefly explain.

**Observation** Based on above 3 model, temperature is better predictor compared to other 2 predictors humidity  and wind. the RMSE value of temperature model is lower compared to others at ~5%. Also R2 value is higher compare to other model and explains 60% of variability of response data in the model compared to 19% and less than ~1% for humidity and wind respectively. Therefore based on these 3 models, temperature is better predictor compared to humidity and wind.
***